---
title: 'ICLR 2025 ML papers'
date: 2025-02-18
permalink: /posts/2025/02/iclr2025/
tags:
  - Machine Learning
  - Large Language Models
---
I am curating (mainly) ICLR '25 submitted papers related to hyperparameter tuning of large-scale training.

|*Title*  |*Summary*  |
|---|---|
|[Scaling Optimal LR Across Token Horizons](https://openreview.net/forum?id=WYL4eFLcxG)|${\rm LR} \propto N^{-0.23}T^{-0.32}$ (fixed batch size)|
|[How Does Critical Batch Size Scale in Pre-training?](https://openreview.net/forum?id=JCiF03qnmi)|${\rm crit. BS} \propto T$ (fixed LR)|
|[Time Transfer: On Optimal Learning Rate and Batch Size In The Infinite Data Limit](https://openreview.net/forum?id=MLhquJb1qN)|Relations of BS, LR, and $T$ are complicated|
|[How to set AdamWâ€™s weight decay as you scale model and dataset size](https://arxiv.org/abs/2405.13698)|the "timescale" 1/(LR * WD) should be constant|
